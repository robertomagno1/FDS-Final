{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Definisci il periodo di tempo\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Scarica i dati per Tesla\n",
    "tsla = yf.download('TSLA', start=start_date, end=end_date)\n",
    "# Scarica i dati per Coca-Cola\n",
    "ko = yf.download('KO', start=start_date, end=end_date)\n",
    "\n",
    "def calculate_volatility(data):\n",
    "    # Calcola i rendimenti giornalieri\n",
    "    data['Daily_Return'] = data['Close'].pct_change()\n",
    "    # Calcola la volatilità come la deviazione standard dei rendimenti giornalieri\n",
    "    data['Volatility'] = data['Daily_Return'].rolling(window=30).std() * (252**0.5)\n",
    "    return data\n",
    "\n",
    "tsla = calculate_volatility(tsla)\n",
    "ko = calculate_volatility(ko)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Definisci il periodo di tempo\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Scarica i dati per Tesla (TSLA)\n",
    "tsla = yf.download('TSLA', start=start_date, end=end_date)\n",
    "# Scarica i dati per Coca-Cola (KO)\n",
    "ko = yf.download('KO', start=start_date, end=end_date)\n",
    "\n",
    "# Funzione per creare tabelle annuali con prezzi di chiusura\n",
    "def create_annual_close_table(data, asset_name):\n",
    "    # Rimuovi le osservazioni con valori nulli o NaN\n",
    "    data = data.dropna(subset=['Close'])\n",
    "    years = data.index.year.unique()\n",
    "    annual_data = []\n",
    "\n",
    "    for year in years:\n",
    "        yearly_data = data[data.index.year == year]\n",
    "        close_prices = yearly_data['Close'].tolist()\n",
    "        annual_data.append([year] + close_prices)\n",
    "\n",
    "    annual_df = pd.DataFrame(annual_data)\n",
    "    annual_df.columns = ['Year'] + [f'Day_{i+1}' for i in range(len(annual_df.columns) - 1)]\n",
    "    return annual_df\n",
    "\n",
    "# Creazione delle tabelle annuali per i prezzi di chiusura\n",
    "tsla_close_annual = create_annual_close_table(tsla, 'TSLA')\n",
    "ko_close_annual = create_annual_close_table(ko, 'KO')\n",
    "\n",
    "# Esporta le tabelle annuali in formato CSV\n",
    "tsla_close_annual.to_csv('/Users/roberto/Desktop/UNI 3^ ANNO/tesi/dataset/data_tsla.csv', index=False)\n",
    "ko_close_annual.to_csv('/Users/roberto/Desktop/UNI 3^ ANNO/tesi/dataset/data_kk.csv', index=False)\n",
    "\n",
    "print(\"Tabelle annuali dei prezzi di chiusura create e salvate in file CSV separati con successo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora , tramite apposita funzione in matlab ho creato spettrogrammi per rappresentare l'informazione sotto forma di segnale per ciascuna stock, per ogni anno.. cosi ho creato molteplici immagini per addestrare il modello CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "procedo anche con data augmentation per prevenire overfitting aumentando rumore e nunerosità delle immagini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Percorsi delle directory di origine e destinazione\n",
    "source_dir_tsla = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/tsla'\n",
    "source_dir_ko = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/ko'\n",
    "augmented_dir_tsla = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/tsla_augmentated'\n",
    "augmented_dir_ko = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/ko_augmentated'\n",
    "\n",
    "# Creazione delle directory se non esistono\n",
    "os.makedirs(augmented_dir_tsla, exist_ok=True)\n",
    "os.makedirs(augmented_dir_ko, exist_ok=True)\n",
    "\n",
    "# Funzione per applicare data augmentation e salvare le immagini aumentate\n",
    "def augment_images(source_dir, augmented_dir, prefix, num_augmented_images=100):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    for img_name in os.listdir(source_dir):\n",
    "        img_path = os.path.join(source_dir, img_name)\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        \n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1, save_to_dir=augmented_dir, save_prefix=prefix, save_format='png'):\n",
    "            i += 1\n",
    "            if i >= num_augmented_images:\n",
    "                break\n",
    "\n",
    "# Applicare data augmentation alle immagini di TSLA e KO\n",
    "augment_images(source_dir_tsla, augmented_dir_tsla, prefix='TSLA')\n",
    "augment_images(source_dir_ko, augmented_dir_ko, prefix='KO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trasfromazioni su spettrogrammi :...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "costruzione del modello di Deep learning per la classificazione delle stock in base a volatilità/rischio-rendimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilizziamo metal di apple per accelerare addestramento tramite l'utilizzo di gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-14.4.1-arm64-arm-64bit\n",
      "PyTorch Version: 2.3.1\n",
      "\n",
      "Python 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Pandas 2.2.2\n",
      "Scikit-Learn 1.4.2\n",
      "NVIDIA/CUDA GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Target device is mps\n",
      "Stai utilizzando il dispositivo: mps\n"
     ]
    }
   ],
   "source": [
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"NVIDIA/CUDA GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")\n",
    "\n",
    "print(\"Stai utilizzando il dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controllo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/augmentated'\n",
    "test_dir = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/test'\n",
    "\n",
    "## Definisci i generatori di dati\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Carica le immagini di addestramento e test\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modello "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Image dimensions and preprocessing\n",
    "img_width, img_height = 224, 224  # Resize to a smaller shape for faster processing\n",
    "\n",
    "train_data_dir = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/train'\n",
    "validation_data_dir = '/Users/roberto/Desktop/UNI 3^ ANNO/tesi/spettrogrammi/test'\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1894 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'  # Binary classification (KO vs TSLA)\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 3. Define the CNN Model\n",
    "model = Sequential()\n",
    "\n",
    "# First Convolutional Layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second Convolutional Layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third Convolutional Layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten and Fully Connected Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary output (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 781ms/step - accuracy: 0.5148 - loss: 1.2842 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/59\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 496ms/step - accuracy: 0.6250 - loss: 0.6920"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:53:45.512948: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6250 - loss: 0.6920 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 3/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 751ms/step - accuracy: 0.4944 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m 1/59\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 498ms/step - accuracy: 0.5000 - loss: 0.6931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:54:35.719552: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 758ms/step - accuracy: 0.4931 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 6/10\n",
      "\u001b[1m 1/59\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 515ms/step - accuracy: 0.4375 - loss: 0.6934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:55:26.335069: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4375 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 761ms/step - accuracy: 0.4704 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m 1/59\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 497ms/step - accuracy: 0.4062 - loss: 0.6936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:56:17.312343: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4062 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 749ms/step - accuracy: 0.4900 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 10/10\n",
      "\u001b[1m 1/59\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 492ms/step - accuracy: 0.5625 - loss: 0.6928"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:57:07.456992: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5625 - loss: 0.6928 - val_accuracy: 0.5000 - val_loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# 5. Train the model\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - accuracy: 0.5000 - loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate the model\n",
    "scores = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {scores[1] * 100}%\")\n",
    "\n",
    "# Save the model for future use\n",
    "model.save('stock_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
